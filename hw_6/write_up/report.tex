\documentclass{article}

\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}

\begin{document}
\title{OpenMPI \textit{n-body} Simulation}
\author{Cyrus Ramavarapu}
\renewcommand{\today}{12 November 2016}
\maketitle

\section*{Intro:}
A common scientific problem is to determine how a system
containing \textit{n} interacting bodies will evolve over
time.  Commonly referred to as the \textit{n-body} problem,
it has found wide spread use in physics, chemistry, biology
and even finance.  However, despite the importance of this
problem, analytical solutions are impossible to achieve
except for a few configurations where $N > 2$.  As a result,
solutions are approximated through simulation where 
interactions are modelled using parameterizable force-fields
to provide the necessary flexibility to effectively capture
real phenommena.\\\\
To effectively perform these simulations at a sufficiently
large scale and in a reasonable amount of time, parallel
execution is often used to distribute the problem over
large computer clusters.  One method to achieve this degree
of massive parallelization is to use the a distributed
memory API.  This report describes the use of one such API,
OpenMPI, to perform a basic \textit{n-body} particle simulation.
In addition to describing the parallelization strategy using
OpenMPI, the efficiency of the implementation and speed up 
observed in the simulation will be analyzed.
\section*{Problem and Strategy:}
A key aspect of any \textit{n-body} simulation is the choice
of the force field which will define how the particles in the 
simulation will interact.  For didactic reasons, a very basic
\textit{van der Waals} force field was chosen using the following
equations for all particle-particle interactions.
\[
f = \frac{A}{r^6} + \frac{B}{r^{12}}
\]
\[
r = \sqrt{r_x^2 + r_y^2}
\]
\[
r_x = x_1 - x_2
\]
\[
r_y = y_1 - y_2
\]
\[
f_x = \frac{f\times r_x}{r}
\]
\[
f_y = \frac{f\times r_y}{r}
\]
Furthermore, the paramters $A$ and $B$ were given.\\\\
When considering the different approaches to parallelize this
problem using OpenMPI, it was assumed that the simulation
will contain an arbitrary number of particles that will need
to be distributed over an \textit{odd} number of processors.
Given these assumptions an obvious choice to distribute the 
particles over the processors was the \textit{MPI\_Scatter}
command.  This command attempts to evenly distribute
\textit{n} particles over $n$ processors.\\\\
Once the particles were distributed over all the processors,
the problem was then to determine an efficient way to
calculate particle particle interactions.   It was initially
realized that duplicating the particles on each processor into
two sets, \textit{locals} and \textit{remotes}, would reduce
the number of calculations required in half because remotes
would only have to interact with half the local sets, whereas
the locals would interact with the other half of the remotes.
This approach was ammenable to a \textit{ring-algorithm}
where each processor would send its \textit{remotes} to
its neighboring processors in a circular manner.  This 
continues for $(n-1)/2$ iterations, and then the \textit{remotes}
are returned to the original processor.  To implement this algorithm,
\textit{MPI\_Send} and \textit{MPI\_Recv} were heavily used to
send the remote particles to the necessary processor.\\\\
Once all the particles completed the necessary calculations on
each processor, to collect all the particles and display results,
the corresponding command to \textit{MPI\_Scatter}, \textit{MPI\_Gather}.
This command gathers all the particles on a single processor
and then displays the results. 
\newpage
\section*{Results:}
Upon completing the simulation for 500 particles (\textit{N=500})
the following times were runtimes were observed for process counts
of 3, 15, and 63.\\\\
\begin{center}
\begin{tabular}{llll}
Processes & Run & Time (s)     & Avg. Time (s)            \\ \hline
3    & 1   & 0.0019 &                \\ 
& 2   & 0.0019 &                \\
& 3   & 0.0019 & 0.0019 \\ \hline
15   & 1   & 0.0126 &                \\
& 2   & 0.0045 &                \\
& 3   & 0.2898  & 0.1023   \\ \hline
63   & 1   & 0.0860  &                \\
& 2   & 0.0816 &                \\
& 3   & 0.0652 & 0.0776 
\end{tabular}
\end{center}

Given that the serial execution for this problem took 27.9050 s,
the speed-up and efficiency were calculated for each of the different
process counts using the following formulas.
\[
Speed Up = \frac{Time_{serial}}{Time_{parallel}}
\]
\[
Efficiency = \frac{Speed Up}{processes}
\]
\begin{center}
\begin{tabular}{lll}
Processes & Speed Up    & Efficiency  \\ \hline
3         & 14566.7236 & 4855.5745 \\ \hline
15        & 272.6099 & 18.1739 \\ \hline
63        & 359.3950 & 5.7046
\end{tabular}
\end{center}

The trend in efficiency and speed up can be observed more clearly
when displayed on semilog plots.

\begin{center}
\includegraphics[scale=0.5]{speed_up.png}
\end{center}

\begin{center}
\includegraphics[scale=0.5]{efficiency.png}
\end{center}

\section*{Discussion:}
Based upon the speedup and efficiency plots, it is observed that
for the size of the system (\textit{N=500}) the greatest speed up
is achieved using a small number of processes.  This is likely
due to the overhead required to initialize the system for a larger
number of processes.
The effiiency results follow a similar trend, with the highest
efficiency being observed for trial run with the fewest number
of processes.\\\\
Although, both the speed up and efficiency results appear to 
support that this problem is best solved using few processes,
this is quite likely the result of the system size.  Future 
studies should consider how the system size impacts the
speed up and efficiency.  Furthermore, it will also be interesting
to explore the effect of forcefield on the parallelizability of 
the problem. 


\end{document}
